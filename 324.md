# Introduction to DevOps
<sup>Internal reference: topics/01-1.md</sup>

## What is DevOps?
The term __DevOps__ represents the combination of __Development (Dev)__ and __Operations (Ops)__. 
DevOps culture is a set of practices that reduce the barriers between developers, who want to innovate and deliver faster, and operations, who want to guarantee the stability of production systems and the quality of the system changes they make.
DevOps culture is also the __extension of agile processes (Scrum, XP, and so on)__, which makes it possible to reduce delivery times and already involves developers and business teams. However, they are often hindered because of the non-inclusion of Ops in the same teams.

Now some __key factors__ to facilitate collaboration and to improve communication between Dev and Ops:
  * Frequent application deployments with integration and continuous delivery (called CI/CD).
  * Implementation and automation of tests, with a process focused on behavior-driven design (BDD) or test-driven design (TDD).
  * Implementation of collecting feedback from users.
  * Monitoring applications and infrastructure.

## Organizational prerequisites
__The way your organization works has a high impact on the success of introducing the CD process__. It's a bit similar to introducing Scrum. Many organizations would like to use the Agile process, but they don't change their culture. You can't use Scrum in your development team unless the organization's structure has been adjusted for that. For example, you need a product owner, stakeholders, and a management team that understands that no requirement changes are possible during the sprint. Otherwise, even with good intentions, you won't make it. The same applies to the CD process; it requires you to adjust how the organization is structured. Let's have a look at three aspects: the DevOps culture, a client in the process, and business decisions.

## DevOps culture
A long time ago, when software was written by individuals or micro teams, there was no clear separation between development, quality assurance, and operations. A person developed the code, tested it, and then put it into production. If anything went wrong, the same person investigated the issue, fixed it, and redeployed it to production. The way the development process is organized changed gradually: systems became larger and development teams grew. Then, engineers started to become specialized in one area. This made perfect sense as specialization caused a boost in productivity. However, the side effect was the communication overhead. This is especially visible if developers, QAs, and operations are in separate departments in the organization, sit in different buildings, or are outsourced to different countries. This organizational structure is not good for the CD process. We need something better; we need to adopt the DevOps culture.

DevOps culture means, in a sense, going back to the roots. A team is responsible for all three areas, which are shown in the following diagram:

{{:en:modul:m324_aws:topics:01_01-devops-culture.png?600|Fig-01: DevOps culture}}

The __benefits of establishing a DevOps culture__ within an enterprise are as follows:

  * Better collaboration and communication in teams, which has a human and social impact within the company
  * Shorter lead times (Lieferzeiten) to production, resulting in better performance and end user satisfaction
  * Reduced infrastructure costs with IaC (Infrastructure as Code)
  * Significant time saved with iterative cycles that reduce application errors and automation tools that reduce manual tasks, so teams focus more on developing new functionalities with added business value.

## Implementing CI/CD and continuous deployment
Key practices of DevOps are:
  * Continuous integration (CI)
  * Continuous delivery (CD)
  * Continuous deployment

Let's look in the following chapters at each of these practices in detail.

## Continuous integration (CI)
Continuous integration is an automatic process that allows you to check the completeness of an application's code every time a team member makes a change. This verification must be done as quickly as possible.
To set up CI, it is necessary to have a Source Code Manager (SCM) that will centralize the code of all members. This code manager can be of any type (i.e. Git or SVN). It's also important to have an automatic build manager (CI server) that supports continuous integration i.e. Jenkins, GitLab CI, TeamCity, GitHub Actions.

A CI-server will retrieve the code and then do the following:
  -Build the application package (compilation, file transformation etc.)
  -Perform unit tests (with code coverage)

This diagram below shows the cyclical steps of continuous integration. This includes the code being pushed into the SCM by the team members and the build and test being executed by the CI server. The purpose of this process is to provide rapid feedback to members.

{{:en:modul:m324_aws:topics:01_02-ci-pipeline.png?600|Fig-02: CI pipeline}}

## Continuous delivery (CD)
Once CI has been completed, the next step is to deploy the application automatically in one or more non-production environments (so called ''staging''). This process is called continuous delivery (CD).

CD often starts with an application package being prepared by CI, which will be installed based on a list of automated tasks. These tasks can be of any type: unzip, stop and restart service, copy files, replace configuration, and so on. The execution of functional and acceptance tests can also be performed during the CD process.

Unlike CI, CD aims to test the entire application with all of its dependencies (i.e. a microservice application composed of several services and APIs). It is important that the package that's generated during CI is the same one that will be installed on all environments, and this should be the case until production. There may be configuration file transformations that differ depending on the environment, but the application code (binaries, DLL, Docker images, and JAR) must remain unchanged.

The tools that are set up for CI/CD are often a
  * ''package manager'': This constitutes the storage space of the packages generated by CI and recovered by CD. These managers must support feeds, versioning, and different types of packages. There are several on the market (Nexus, ProGet, Artifactory).
  * ''configuration manager'': Most CD tools include a configuration mechanism with a system of variables.

In CD, deploying the application in each staging environment can be triggered ...
  * ''automatically'', following a successful execution in a previous environment.
  * ''manually'', for sensitive environments such as the production environment, following manual approval by the person responsible for validating the proper functionality of the application in an environment.

{{:en:modul:m324_aws:topics:01_03-cicd-pipeline.png?600|Fig-03: Continuous delivery (CD) pipeline}}

The preceding diagram shows that the CD process is a continuation of the CI process. It represents the chain of CD steps, which are automatic for staging environments but manual for production deployments. The package is generated by CI and stored in a package manager, and that it is the same package that is deployed in different environments.

## Continuous deployment
Continuous deployment is an extension of CD, but this time, with a process that automates the entire CI/CD pipeline from the moment the developer commits their code to deployment in production through all the verification steps.

{{:en:modul:m324_aws:topics:01_04-cicdeploy-pipeline.png?600|Fig-04: Continuous deployment pipeline}}

This practice is rarely implemented in enterprises because it requires a variety of tests (unit, functional, integration, performance, and so on) to be covered for the application. Successfully executing these tests is sufficient to validate the proper functionality of the application regarding all of these dependencies. However, it also allows you to automatically deploy to a production environment without any approval action required.

The continuous deployment process must also take into account all the steps to restore the application in the event of a production problem.

----

Based on the book:
"Continuous Delivery with Docker and Jenkins, 3rd Edition - Third Edition By Leszko"

----

[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}}]] Daniel Garavaldi


# Introduction to Jenkins
<sup>Internal reference: topics/02-1.md</sup>

## What is Jenkins?
Jenkins is an open source automation server written in Java. With very active community-based support and a huge number of plugins, it is one of the most popular tools for implementing continuous integration and continuous delivery processes. Formerly known as Hudson, it was renamed after Oracle bought Hudson and decided to develop it as proprietary software. Jenkins was forked from Hudson but remained open source under the MIT license. It is highly valued for its simplicity, flexibility, and versatility.

## Characteristics of Jenkins
Jenkins outshines other continuous integration tools and is the most widely used software of its kind. That's all possible because of its features and capabilities.

Let's walk through the most interesting parts of Jenkins:
  * ''Language-agnostic'': Jenkins has a lot of plugins, which support most programming languages and frameworks. Moreover, since it can use any shell command and any software, it is suitable for every automation process imaginable.
  * ''Extensible by plugins'': Jenkins has a great community and a lot of available plugins (over a thousand). It also allows you to write your own plugins in order to customize Jenkins for your needs.
  * ''Supports most Source Control Management (SCM) tools'': Jenkins integrates with virtually every source code management or build tool that exists.
  * ''Distributed'': Jenkins has a built-in mechanism for the master/agent mode, which distributes its execution across multiple nodes, located on multiple machines. It can also use heterogeneous environments; for example, different nodes can have different operating systems installed.
  * ''Simplicity'': The installation and configuration process is simple. There is no need to configure any additional software or the database. It can be configured completely through a GUI, XML, or Groovy scripts.
  * ''Code-oriented'': Jenkins pipelines are defined as code. Also, Jenkins itself can be configured using YAML/XML files or Groovy scripts. That allows you to keep the configuration in the source code repository and helps in the automation of the Jenkins configuration.

----
//agnostic: not specific to a particular programming language aligned (ausgerichtet)//

Based on the book:
"Continuous Delivery with Docker and Jenkins, 3rd Edition - Third Edition By Leszko"

----

[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}}]] Daniel Garavaldi

# Jenkins architecture
<sup>Internal reference: topics/02-2.md</sup>

## Master and agents
Jenkins server (master) delegates builds and execution tasks to agent (slave) instances. 

{{:en:modul:m324_aws:topics:02_01-jk-master-agent.jpg?600|Fig-01: Jenkins master-agent interaction}}

Jenkins controller (master) is responsible for the following:
  * Receiving build triggers (for example, after a commit to GitHub)
  * Sending notifications (for example, email or Slack messages sent after a build failure)
  * Handling HTTP requests (interaction with clients)
  * Managing the build environment (orchestrating the job executions on agents)

The build agent is a machine that takes care of everything that happens after the build has started.

## Scalability
When Jenkins instance become overloaded or unresponsive you can scaling it up, either vertical or horizontal.

### Vertical scaling
Vertical scaling means that more resources (more RAM, CPU cores, HDD drives etc.) are applied to a machine. For example: having a single Jenkins master set on ultra-efficient hardware is a maintenance advantage. Any upgrades, scripts, security settings, role assignments, or plugin installations have to be done in one place only.

### Horizontal scaling
Horizontal scaling means that more master instances are launched with some significant advantages:
  * Master machines don't need to be special, in terms of hardware.
  * Different teams can have different Jenkins settings (for example, different sets of plugins).
  * Teams usually feel better and work with Jenkins more efficiently if the instance is their own.
  * If one master instance is down, it does not impact the whole organization.
  * The infrastructure can be segregated into standard and mission-critical.

## Test and production instances
How to test the Jenkins upgrades, new plugins, or pipeline definitions? Jenkins is critical to the whole company. It guarantees the quality of the software and, in the case of continuous delivery, deploys to the production servers. That is why it needs to be highly available, and it is definitely not for the purpose of testing. It means __there should always be two instances of the same Jenkins infrastructure__ – test and production.

## Sample architecture
We already know that there should be agents and (possibly multiple) masters and that everything should be duplicated in the test and production environments. However, what would the complete picture look like?
Let's look at the example of Netflix:

{{:en:modul:m324_aws:topics:02_02-jk-example.jpg?600|Fig-02: Jenkins infrastructure from Netflix (2012)}}

They have test and production master instances, with each of them owning a pool of agents and additional ad hoc agents. Altogether, it serves around 2,000 builds per day. One part of their infrastructure is hosted on AWS and another part is on their own servers.

## Setting agents (slave)
Agents always communicate with the Jenkins master using one of the protocols: __SSH (sshd)__ or __Java web__ start. At a higher level, we can attach agents to the master in various ways:

  * **Static versus dynamic**: The simplest option is to add agents permanently in the Jenkins master. The drawback of such a solution is that we always need to manually change something if we need more (or fewer) agent nodes. A better option is to dynamically provision agents as they are needed.
  * **Specific versus general-purpose**: Agents can be specific (for example, different agents for the projects based on Java 8 and Java 11) or general-purpose (an agent acts as a Docker host and a pipeline is built inside a Docker container).

These differences resulted in four common strategies for how agents are configured:
  * Permanent agents
  * Permanent Docker host agents
  * Jenkins Swarm agents
  * Dynamically provisioned Docker agents (focus)
  * Dynamically provisioned Kubernetes agents

Let's focus on one strategy. Other strategies can be dived in -> [[https://www.jenkins.io/doc/book/using/using-agents/|here]].

### Dynamically provisioned Docker agents
Another option is to set up Jenkins to dynamically create a new agent each time a build is started. Such a solution is obviously the most flexible one, since the number of agents dynamically adjusts to the number of builds. 

{{:en:modul:m324_aws:topics:02_03-jk-dyn-docker-agents.jpg?600|Fig-03: Dynamically provisioned Docker agents}}

Docker agent mechanism used step by step:
  -When the Jenkins job is started, the master runs a new container from the ''jenkins/agent'' image on the agent Docker host.
  -The ''jenkins/agent'' container starts the Jenkins agent and attaches it to the Jenkins master's nodes pool.
  -Jenkins executes the pipeline inside the ''jenkins/agent'' container.
After the build, the master stops and removes the agent container.
Information

Running the Jenkins master as a Docker container is independent of running Jenkins agents as Docker containers. It's reasonable to do both, but any of them will work separately.

Custom Jenkins images
So far, we have used Jenkins images pulled from the internet. We used jenkins/jenkins for the master container and jenkins/agent (or jenkins/inbound-agent or jenkins/ssh-agent) for the agent container. However, you may want to build your own images to satisfy the specific build environment requirements. In this section, we will cover how to do it.

## Custom Jenkins images
Jenkins images can be pulled from the internet
  * ''jenkins/jenkins'' for the master container
  * ''jenkins/agent'' (or jenkins/inbound-agent or jenkins/ssh-agent) for the agent container. 

However, you may want to build your own images to satisfy the specific build environment requirements.

### Building a custom Jenkins agent
Let's start with the agent image because it's more frequently customized. The build execution is performed on the agent, so it's the agent that needs to have the environment adjusted to the project we want to build – for example, it may require the Python interpreter if our project is written in Python. The same applies to any library, tool, or testing framework, or anything that is needed by the project.

There are four steps to building and using the custom image:

  -Create a Docker file.
  -Build the image.
  -Push the image into a registry.
  -Change the agent configuration on the master.

### Building the Jenkins master
Why would we also want to build our own master image? 
Imagine the following scenario: your organization scales Jenkins horizontally, and each team has its own instance. There is, however, some common configuration – for example, a set of base plugins, backup strategies, or the company logo. Then, repeating the same configuration for each of the teams is a waste of time. So, we can prepare the shared master image and let the teams use it.
Jenkins is natively configured using ''XML files'', and it provides the ''Groovy-based DSL'' language to manipulate them. That is why we can add the Groovy script to the Dockerfile in order to manipulate the Jenkins configuration. 
All possibilities are well described on the -> [[https://github.com/jenkinsci/docker|GitHub-Page]].

----
Based on the book:
"Continuous Delivery with Docker and Jenkins, 3rd Edition - Third Edition By Leszko"

----

[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}}]] Daniel Garavaldi

# Introduction to Jenkins pipeline
<sup>Internal reference: topics/03-1.md</sup>

## Introduction
**A pipeline is a sequence of automated operations** that usually represents a part of the software delivery and quality assurance process. It can be seen as a chain of scripts that provide the following additional benefits:
  * ''Operation grouping'': Operations are grouped together into stages (also known as gates or quality gates) that introduce a structure into a process and clearly define a rule – if one stage fails, no further stages are executed.
  * ''Visibility'': All aspects of a process are visualized, which helps in quick failure analysis and promotes team collaboration.
  * ''Feedback'': Team members learn about problems as soon as they occur so that they can react quickly

## The pipeline structure
A Jenkins pipeline consists of two kinds of elements – a stage and a step. The following diagram shows how they are used:

{{:en:modul:m324_aws:topics:02_01.png?600|Fig-01: The Jenkins pipeline structure}}

The following are the basic pipeline elements:

Step: A single operation that tells Jenkins what to do – for example, check out code from the repository and execute a script  
Stage: A logical separation of steps that groups conceptually distinct sequences of steps – for example, build, test, and deploy, used to visualize the Jenkins pipeline progressInformationTechnically, it's possible to create parallel steps; however, it's better to treat them as an exception that is only used for optimization purposes.

## A multi-stage Hello World
As an example, let's extend the Hello World pipeline to contain two stages:

```jenkins
pipeline {
     agent any
     stages {
          stage('First Stage') {
               steps {
                    echo 'Step 1. Hello World'
               }
          }
          stage('Second Stage') {
               steps {
                    echo 'Step 2. Second time Hello'
                    echo 'Step 3. Third time Hello'
               }
          }
     }
}
```


The pipeline has no special requirements in terms of environment, and it executes three steps inside two stages. When we click on __Build Now__, we should see a visual representation:

{{:en:modul:m324_aws:topics:02_02.png?600|Fig-02: The multi-stage pipeline build}}

The pipeline succeeded, and we can see the step execution details by clicking on the console. If any of the steps failed, processing would stop, and no further steps would run. Actually, the sole reason for a pipeline is to prevent all further steps from execution and visualize the point of failure.

----
Based on the book:
"Continuous Delivery with Docker and Jenkins, 3rd Edition - Third Edition By Leszko"

----

[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}}]] Daniel Garavaldi

# Jenkins' Commit Pipeline
<sup>Internal reference: topics/03-3.md</sup>

## Introduction
The most basic continuous integration process is called a commit pipeline. This classic phase, as its name indicates, starts with commit (or push in Git) to the main repository and results in a report about the build success or failure. Since it runs after each change in the code, the build should take no more than 5 minutes and should consume a reasonable amount of resources. The commit phase is always the starting point of the continuous delivery process and provides the most important feedback cycle in the development process – constant information if the code is in a healthy state.

The commit phase works as follows: a developer checks in the code to the repository, the continuous integration server detects the change, and the build starts. The most fundamental commit pipeline contains three stages:

  * Checkout: This stage downloads the source code from the repository.
  * Compile: This stage compiles the source code.
  * Unit test: This stage runs a suite of unit tests.

## Jenkins file
So far, we've created all the pipeline code directly in Jenkins. This is, however, not the only option. We can also put the pipeline definition inside a file called ''Jenkins file'' and commit it to the repository, together with the source code. This method is even more consistent because the way your pipeline looks is strictly related to the project itself.

For example, if you don't need the code compilation because your programming language is interpreted (and not compiled), you won't have the Compile stage. The tools you use also differ, depending on the environment. We are going to use ''Node.js''. However, in the case of a project written in Python, you can use PyBuilder. This leads to the idea that the pipelines should be created by the same people who write the code – the developers. Also, the pipeline definition should be put together with the code, in the repository.

This approach brings immediate benefits, as follows:

  * In the case of a Jenkins failure, the pipeline definition is not lost (because it's stored in the code repository, not in Jenkins).
  * The history of the pipeline changes is stored.
  * Pipeline changes go through the standard code development process (for example, they are subjected to code reviews).
  * Access to the pipeline changes is restricted in exactly the same way as access to the source code.

Let's see how it all looks in practice by creating a Jenkins file.

## Creating the Jenkins file
We can create the Jenkins file and push it into our Repository. Its content is almost the same as the commit pipeline we wrote. The only difference is that the checkout stage becomes redundant because Jenkins has to first check out the code (together with Jenkins file) and then read the pipeline structure (from Jenkins file). This is why Jenkins needs to know the repository address before it reads Jenkins file.

{{:en:modul:m324_aws:topics:03_03.png?600|Fig-01: The Jenkins file pipeline configuration}}

----
Based on the book:
"Continuous Delivery with Docker and Jenkins, 3rd Edition - Third Edition By Leszko"

----

[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}}]] Daniel Garavaldi

# Jenkins' Pipeline syntax
<sup>Internal reference: topics/03-2.md</sup>

## Pipeline syntax
In this course we use the declarative syntax that is recommended for all new projects. The other options are a Groovy-based DSL and XML (created through the web interface).
The declarative syntax was designed to make it as simple as possible to understand the pipeline, even by people who do not write code on a daily basis. This is why the syntax is limited only to the most important keywords.

Let's try an example and read the comments carefully:

```jenkins
pipeline {
     //Uses any available agent
     agent any
     //Triggers every 15 minutes     * see more https://linuxhandbook.com/crontab/
     triggers { cron('H/15   * * * *') }
     //Stops if the execution takes more than 5 minutes
     options { timeout(time: 5) }
     //Asks for the Boolean input parameter before starting
     parameters { 
          booleanParam(name: 'DEBUG_BUILD', defaultValue: true, 
          description: 'Is it the debug build?') 
     }
     stages {
          stage('Example') {
               //Sets Node-Backend as the NAME environment variable
               environment { NAME = 'Node-Backend' }
               when { expression { return params.DEBUG_BUILD } } 
               steps {
                    echo Building $NAME
                    script {
                         def browsers = ['chrome', 'firefox']
                         for (int i = 0; i < browsers.size(); ++i) {
                              echo Testing the ${browsers[i]} browser.
                         }
                    }
               }
          }
     }
     post { always { echo 'I will always say Hello again!' } }
}
```

## What are Sections?
Sections define the pipeline structure and usually contain one or more directives or steps. They are defined with the following keywords:

  * ''Stages'': This defines a series of one or more stage directives.
  * ''Steps'': This defines a series of one or more step instructions.
  * ''Post'': This defines a series of one or more step instructions that are run at the end of the pipeline build; they are marked with a condition (i.e. always, success, or failure) and are usually used to send notifications after the pipeline build.
  * ''Agent'': This specifies where the execution takes place and can define label to match the equally labeled agents, or docker to specify a container that is dynamically provisioned to provide an environment for the pipeline execution.

## What are Directives?
Directives express the configuration of a pipeline or its parts:

  * ''Triggers'': This defines automated ways to trigger the pipeline and can use cron to set the time-based scheduling.
  * ''Options'': This specifies pipeline-specific options – for example, timeout (the maximum time of a pipeline run) or retry (the number of times the pipeline should be rerun after failure).
  * ''Environment'': This defines a set of key values used as environment variables during the build.
  * ''Parameters'': This defines a list of user-input parameters.
  * ''Stage'': This allows for the logical grouping of steps.
  * ''When'': This determines whether the stage should be executed, depending on the given condition.
  * ''Tools'': This defines the tools to install and put on PATH.
  * ''Input'': This allows us to prompt the input parameters.
  * ''Parallel'': This allows us to specify stages that are run in parallel.
  * ''Matrix'': This allows us to specify combinations of parameters for which the given stages run in parallel.

## What are Steps?
Steps are the most fundamental part of the pipeline. They define the operations that are executed, so they actually tell Jenkins what to do:

  * ''sh'': This executes the shell command; actually, it's possible to define almost any operation using sh.
  * ''custom'': Jenkins offers a lot of operations that can be used as steps (for example, echo); many of them are simply wrappers over the sh command used for convenience. Plugins can also define their own operations.
  * ''script'': This executes a block of Groovy-based code that can be used for some non-trivial scenarios where flow control is needed.

The complete specification of the available steps can be found at -> [[https://jenkins.io/doc/pipeline/steps/|here]].

----
Based on the book:
"Continuous Delivery with Docker and Jenkins, 3rd Edition - Third Edition By Leszko"

----

[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}}]] Daniel Garavaldi

# Docker in a Docker (DinD)
<sup>Internal reference: topics/04-1.md</sup>

## Introduction
Here are a few use cases to run docker inside a docker container.
  * One potential use case for docker in docker is for the CI/CD pipeline, where you need to build and push docker images to a container registry after a successful code build.
  * Modern CI/CD systems support Docker-based agents or runners where you can run all the build steps inside a container and build container images inside a container agent.
  * Building Docker images with a VM is pretty straightforward. However, when you plan to use Jenkins Docker-based dynamic agents for your CI/CD pipelines, docker in docker comes as a must-have functionality.
  * Sandboxed environments.
  * For experimental purposes on your local development workstation.

## How it works
DinD creates a child container inside a Docker container. For this, you need either an official docker image with dind tag. The dind image is baked with the required utilities for Docker to run inside a docker container. Or you build it with a Dockerfile with more control for all added packages and configuration (see Exercises in this course)

{{:en:modul:m324_aws:topics:03_01.png?600|Fig-01: Docker in Docker container}}

## Example with ready-to-use image
(1) Create a container named i.e. dind-test with ''docker:dind'' image from Docker-Hub.
```
docker run --privileged -d --name dind-test docker:dind
```

(2) Log in to the container using exec.
```
docker exec -it dind-test /bin/sh
```

(3) When you list the docker images, you should see the Ubuntu image along with other docker images in your host.
```
docker images
```

(4) Now create a Dockerfile inside the test directory.
```
mkdir test && cd test
```

(5) Create a Dockerfile
```
nano Dockerfile
```

(6) Copy the following Dockerfile contents to test the image build from within the container.
```
FROM ubuntu:18.04
LABEL maintainer=Hans Muster <hmuster@example.com>
RUN apt-get update && \
    apt-get -qy full-upgrade && \
    apt-get install -qy curl && \
    curl -sSL https://get.docker.com/ | sh
```

(7) Build the Dockerfile
```
docker build -t test-image .
```

----

[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}}]] Daniel Garavaldi

# How to Use Conditional Constructs in Jenkins Pipeline

<sup>Internal reference: topics/05-1.md</sup>

## Introduction

Effective Jenkins pipelines rely heavily on conditional constructs to control the execution flow. Using these constructs, we get the flexibility to change the flow of execution based on dynamic criteria, such as environment variables, results from previous steps, or any other condition.
The declarative nature of writing pipelines in Jenkins allows us to define the pipeline jobs in a simplified and structured way. In this section, let’s explore different ways to write conditional logic in declarative pipelines.

## *test*-Command with *sh*-Step

Jenkins offers several built-in steps, such as sh, to facilitate writing pipelines conveniently. Further, we can use the sh block to write shell commands. Later, at the execution time, Jenkins executes these shell commands as shell scripts on one of the Jenkins nodes.

Let’s write a simple Jenkins pipeline named job-1 with a build stage that must execute the build steps only when the SKIP_BUILD variable isn’t set:

```
pipeline {
  agent any

  stages {
    stage('build') {
      steps {
        sh """
        test -z \$SKIP_BUILD && echo 'starting to build ...'
        """
      }
    }
  }
}
```

We can see that having access to shell commands allows us to use the test command for the conditional execution of the build steps after it. Further, we must note that the SKIP_BUILD variable is expected to be defined somewhere outside the steps section, so we need to use $ escaping so that Jenkins interprets it as a shell variable.

Next, let’s save our pipeline job and execute it using the “Build Now” button in the sidebar:

```
Started by user admin
[Pipeline] Start of Pipeline
[Pipeline] node
Running on Jenkins in /var/jenkins_home/workspace/job-1
[Pipeline] {
[Pipeline] stage
[Pipeline] { (build)
[Pipeline] sh
+ test -z
+ echo starting to build ...
starting to build ...
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // node
[Pipeline] End of Pipeline
Finished: SUCCESS
```

Since we didn’t define the SKIP_BUILD variable, we expected the “starting to build …” text in the console output, which is precisely the case.

## *if-else* with *sh* Step

Similar to the test command, we can use other shell commands and constructs within the sh block.
Let’s go ahead and write the job-2 Jenkins pipeline that uses an if–else logic to trigger the build stage:

```jenkins
pipeline {
  agent any

  stages {
    stage('build') {
      steps {
        script {
          sh """
            if [ -z \${SKIP_BUILD} ]
            then
            echo starting build ...
            else
            echo skipped build ...
          fi
        """
        }
      }
    }
  }
}
```

Moving on, let’s trigger our job-2 pipeline and check its console output:

```
Started by user admin
[Pipeline] Start of Pipeline
[Pipeline] node
Running on Jenkins in /home/jenkins/workspace/job-2
[Pipeline] {
[Pipeline] stage
[Pipeline] { (build)
[Pipeline] script
[Pipeline] {
[Pipeline] sh
+ [ -z ]
+ echo starting build ...
starting build ...
[Pipeline] }
[Pipeline] // script
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
[Pipeline] // node
[Pipeline] End of Pipeline
Finished: SUCCESS
```

For more examples, see in your Course Repository.

---

[[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png](https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)}}]] Daniel Garavaldi

# What is code coverage?

<sup>Internal reference: topics/06-1.md</sup>

## Introduction

Code coverage is a software testing metric that determines the number of lines of code that is successfully validated under a test procedure. This metric helps to analyze how comprehensively a software is verified.
Developing enterprise-grade software products is the ultimate goal of any software company. However, to accomplish this goal, companies have to ensure that the software they develop meets all the essential quality characteristics, i.e. according to ISO 9126 standard that is – *functionality, usability, reliability, efficiency, security, portability and maintainability*. This can only be possible by thoroughly reviewing the software product.
Along with handing off the software to the QA engineers for bug tracking, it is imperative to analyze, monitor, measure test activities. This means, software testing metrics to evaluate the test suite’s effectiveness and completeness should be considered.
*Code coverage* is one such software testing metric that can help in assessing the test performance and quality aspects of any software.
Such an insight will equally be beneficial to the development and QA team. For developers, this metric can help in dead code detection and elimination. On the other hand, for QA, it can help to check missed or uncovered test cases. They can track the health status and quality of the source code while paying more heed to the uncaptured parts of the code.

## Benefits of Code Coverage

Before we list down the benefits, let’s first burst a few myths. Code coverage analysis can only be used for the validation of test cases that are run on the source code and not for the evaluation of the software product.
Also, it neither evaluates whether the source code is bug-free nor proves if a written code is correct.
Then, why is it important you ask? Here’s why you should care about this analysis:

* *Easy maintenance of code base* – Writing scalable code is crucial to extend the software program through the introduction of new or modified functionalities. However, it is difficult to determine whether the written code is scalable. It can prove to be a useful metric in that context. The analysis report will help developers to ensure code quality is well-maintained and new features can be added with little-to-no efforts.
* *Exposure of bad code* – Continuous analysis will help developers to understand bad, dead, and unused code. As a result, they can improve code-writing practices, which in turn, will result in better maintainability of the product quality.
* *Faster time to market* – With the help of this metric, developers can finish the software development process faster, thereby increasing their productivity and efficiency. As a result, they will be able to deliver more products, allowing companies to launch more software applications on the market in lesser time. This will undoubtedly lead to increased customer satisfaction and high ROI.

## How is it measured?

To calculate the code coverage percentage, simply use the following formula:

```
Code Coverage Percentage =
(
Number of lines of code executed by a testing algorithm
divided by
Total number of lines of code in a system component
)
multiplied by 100.
```

For example:
If the software you are testing contains a total of 100 lines of code and the number of lines of code that is actually validated in the same software is 50, then the code coverage percentage of this software will be 50 percent.

Looking at the example above, you might crave to achieve 100 percent coverage for your software product. You may think, the more the coverage, the better the code quality of any software program. However, this isn’t true. So, what ideal coverage percentage developers and testers should aim for?

## What is an ideal code coverage percent?

Striking 100 percent code coverage means the code is 100 percent bugless. No error indicates that test cases have covered every criterion and requirement of the software application. So, if that’s the case, how do we evaluate if the test scripts have met a wide range of possibilities? What if the test cases have covered the incorrect requirements? What if test cases have missed on some important requirements? So, that drills down to the fact that, if a good software product built on 100 percent irrelevant test case coverage, then the software will undoubtedly compromise on quality.

The only focus and goal of developers and testers should be to write test scripts that aren’t vague. Don’t focus to achieve 100 percent coverage. The analysis should be clubbed with scalable, robust test scripts, covering every functional and non-functional area of the source code.

## Code coverage criteria

To measure the lines of code that are actually exercised by test runs, various criteria are taken into consideration. We have outlined below a few critical coverage criteria that companies use.

* *Function Coverage* – The functions in the source code that are called and executed at least once.
* *Statement Coverage* – The number of statements that have been successfully validated in the source code.
* *Branch or Decision Coverage* – The decision control structures (loops, for example) that have executed fine.
* *Condition Coverage* – The Boolean expressions that are validated and that executes both TRUE and FALSE as per the test runs.
* *Path Coverage* – The flows containing a sequence of controls and conditions that have worked well at least once.

## Summing Up

In this fast-paced technology-driven world, developers and testers have to intensify the rapidity of their software development life-cycles. And to handle tight deadlines, software engineers must build only good code. Hence, good code quality is what every developer or tester is aiming for. With a code coverage analysis report, they can track the proportion of code that worked well under test scenarios. This insight will act like a feedback report, which will help developers to write good and clean source code. This ultimately will result in improved code quality, positively impacting the software quality.

However, depending on coverage metrics solely for assessing code health isn’t a good option. Code coverage analysis and code reviews, along with your QA efforts, can be one powerful way of improving the functionality of code.

---

[[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png](https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)}}]] Daniel Garavaldi

# Code coverage with Jest

<sup>Internal reference: topics/06-2.md</sup>

## Introduction

Popular JavaScript frameworks can use Facebook’s Jest to perform unit tests.
Jest has the Coverage Report feature that allows us to check if our code covers all lines of the files we choose by generating an HTML file that we can open.

## Code coverage criteria in Jest

* *Statements* represent instructions that have been executed at least once during the unit tests.

For example, we can have a line that contains two statements:

```js
var age= 18; console.log(age)
```

This contains a variable declaration statement and a statement that executes the log function that belongs to the console object.

Furthermore:

* *Branches* represent if statements which conditions have been fulfilled at least once during the unit tests.
* *Functions* represent functions that have been called at least once during the unit tests.
* *Lines* represent code lines that have executed at least once during the unit tests.

## Jest code coverage report in your terminal

Seeing your code coverage can be as simple as adding the *--coverage* flag when running your Jest unit tests:

```bash
jest --coverage
```

After you run the coverage command you’ll get a summary report that looks like this:

{{:en:modul:m324_aws:topics:06_01.png?600|Fig-01: Code coverage in Jest on the terminal.}}

By adding *--collectCoverageFrom*, Jest will calculate code coverage for all the files that you specify. Even ones without any tests:

```bash
jest --coverage --collectCoverageFrom='src/**/*.js'
```

{{:en:modul:m324_aws:topics:06_02.png?600|Fig-02: Code coverage for all files.}}

## Jest code coverage report in your browser

When looking at the summary table, it can be very hard to determine where you are missing coverage! A much easier way is to generate an HTML report in the folder you specified with *--coverageDirectory*:

```bash
jest --coverage --coverageDirectory='coverage' --collectCoverageFrom='src/**/*.js'
```

{{:en:modul:m324_aws:topics:06_03.png?600|Fig-03: Code coverage in Jest in the browser.}}

If you open up the index.html file in your browser, you will see lines highlighted in red. These are the lines that are not currently covered by your unit tests.

## Symbols and Numbers

If you open up the index.html file in your browser, you will see lines highlighted in red. These are the lines that are not currently covered by your unit tests.

{{:en:modul:m324_aws:topics:06_04.png?600|Fig-04: Drill down of the report.}}

By analyzing the lines of code we can see *1x* on the left hand side. It means that we executed that part of the code one time during our unit tests. This happens because I only tested one of the functional requirements on my unit tests.

The *I symbol* at line 7 means that this branch has not been entered during unit tests (if path not taken). Similar is the *E symbol* at line 13 means that alternative branch has not been entered during unit tests (else path not taken).

If there is a part of line that is highlighted in yellow, it means that the possible branch is not covered.

## The pitfalls of aiming for 100% coverage

As you increase your code coverage, sometimes it will be too hard to cover certain lines of code with unit tests. Spending your time trying to find a workaround to cover that line of code is never worth it. There are much better things you could be spending your time on than striving for 100% coverage!

Even if you do cover a line of code, there's no guarantee that it will be perfect and bug-free, either. If we take a look at a double function that doubles the number you pass in:

```js
const double = (number) => 2;
```

You could test that double(1) = 2 and that test would pass. You would have 100% code coverage as well. But your function would fail with all other numbers.

Code coverage is useful, but it's important not to use it as the only metric to measure your unit tests. Make sure to keep in mind all the possible edge cases and different scenarios, as code coverage won't pick these up.

---

[[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png](https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)}}]] Daniel Garavaldi

===== Code coverage with SonarQube =====
<sup>Internal reference: topics/06-3.md</sup>
\\
==== Introduction ====
SonarQube is a Java-based open-source code coverage tool. Beside running code coverage, it allows static code analysis to evaluate the reliability and security of a program. With ''SonarQube'', development teams may use fully customizable reports and a dashboard to show the quality of the code in their apps.

This program can analyze the static code of more than 25 languages, including PHP: Hypertext Preprocessor (PHP), Java,.NET, JavaScript, Python, and others. For a complete list, go to the SonarQube docs.

SonarQube also provides code analysis for security issues, code smells, and code duplication, as well as code coverage for unit tests.


\\
==== Constraints of SonarQube ====
As a reminder: Test coverage statistics and test execution reports will show you how much of your code is covered by your test cases.

SonarQube cannot determine coverage by itself. Set up of a third-party coverage tool is therefore required in order to import data into SonarQube. The right ''SonarScanner'' configuration is required in order to integrate code analysis into your build procedure.

\\
==== Getting started ====
The following procedure describes steps to set up SonarQube code coverage using JavaScript
As prerequisites you should have the following components installed on your system:
* NodeJS
* Docker
* A text editor or IDE

\\
=== Step 1: Download and start SonarQube ===
SonarQube must be run on servers or virtual machines because it is an on-premise solution (VMs). Without having to explicitly configure the server on your system, starting up an instance can be replaced by installing a Docker container from the Sonar image.

<code>
docker pull sonarqube:latest
docker run -d --name sonarqube -p 9000:9000 sonarqube:latest
</code>

Once your instance is up and running, you can log in and access the sonarqube instance from your local browser through http://localhost:9000 using System Administrator default credentials.

<code>
login: admin
password: admin
</code>

\\
=== Step 2: Create a new project ===
* As Project type select ''Create a local Project''
* Then set ''Project display name and key'' and the ''main branch name (default: main)''.
* Hit the button ''Next''
* Select ''Follows the instance's default''
* Hit the button ''Create project''

\\
=== Step 3: Analysis method ===
* As analysis method select ''Locally''
* Generate the ''Token name'' (Expiration in 30 days is ok)
* Save the generated token and handle it as password.
* Hit the button ''Continue''
* Run analysis on your project: ''Other'' -> ''Linux''



\\
=== Step 4: Download and unzip the scanner ===
* Download the scanner for your platform.

{{:en:modul:m324_aws:topics:01-sonar.png?direct&400|}}
{{:en:modul:m324_aws:topics:02-sonar.png?direct&400|}}

* Unzip it either in your project directory or in your binary-directory.
Example for Linux on AWS EC2
<code>
wget -N https://binaries.sonarsource.com/<whatever-path>/sonar-scanner-cli-<whatever-version>-linux-x64.zip
sudo apt install unzip
unzip -o sonar-scanner-cli-<whatever-version>-linux-x64.zip
</code>

* Add the ''bin'' directory of the unzipped folder in your ''PATH'' variable and check with command ''whereis''. Example:
<code>
whereis sonar-scanner
sonar-scanner: /home/ubuntu/sonar/bin/sonar-scanner
</code>
* Check before running code coverage that your target app (i.e. traffic light api) has the following node package are installed.

<code>
dependencies: {
...
@types/jest: ^29.5.11,
jest: ^29.7.0,
jest-sonar-reporter: ^2.0.0,
sonarqube-scanner: ^4.2.6,
supertest: ^6.3.3
}
</code>

* Create in your project folder a file ''sonar-project.properties''. Here you can store your settings, especially project-key, source-path, SonarQube host url and token. In the example the code source is in directory ''src'' and unit tests are on the same directory level in directory ''tests''.

<code>
#SonarQube configuration for server connection
sonar.projectKey=??
sonar.host.url=http://localhost:9000
sonar.token=??
sonar.sources=??
sonar.exclusions=
sonar.test=tests
sonar.language=javascript
sonar.scm.disabled=true
sonar.test.inclusions=tests/*.test.js
sonar.javascript.coveragePlugin=lcov
sonar.javascript.lcov.reportPaths=./coverage/lcov.info
sonar.testExecutionReportPaths=./coverage/test-reporter.xml
sonar.sourceEnconding=UTF-8
</code>

\\
=== Step 5: Run the tests ===
* On Linux: Install jest with ''sudo npm i jest-cli -g''
* Run first the code coverage with jest.

<code>
jest --coverage --coverageDirectory='coverage' --collectCoverageFrom='src/**/*.js'
</code>

* Execute the scanner as shown.

<code>
#!/usr/bin/bash
# Attention:
# Change CRLF (Windows) to LF (Unix on AWS) in your Editor
#
sonar-scanner \
-Dsonar.projectKey=<YOUR-PROJECT-KEY> \
-Dsonar.sources=. \
-Dsonar.host.url=<YOUR-HOST-URL> \
-Dsonar.token=<YOUR-TOKEN> \
-Dproject.settings=./sonar-project.properties
</code>

* Analyse the generated report in SonarQube

{{:en:modul:m324_aws:topics:03-sonar.png?direct&800|Sample report from sonar}}



----
Source: [[https://www.aviator.co/blog/implementing-sonarqube-code-coverage-in-a-simple-javascript-application/]].

[[https://creativecommons.org/licenses/by-nc-sa/4.0/|{{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}}]] Daniel Garavaldi